#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/parakeet_ctc/modular_parakeet_ctc.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_parakeet_ctc.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from typing import Callable, Optional, Union

import torch
from torch import nn

from ...activations import ACT2FN
from ...modeling_outputs import BaseModelOutput
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs
from ...utils.generic import can_return_tuple, check_model_inputs
from .configuration_parakeet import ParakeetFastConformerConfig

import math
from typing import Optional, Union

import torch
import torch.nn.functional as F
from torch import nn

from ...modeling_outputs import CausalLMOutput
from ...modeling_utils import PreTrainedModel
from ...utils.generic import can_return_tuple
from ..fastconformer import FastConformerEncoder
from .configuration_parakeet import ParakeetConfig


class ParakeetFastConformerRelPositionalEncoding(nn.Module):
    """Relative positional encoding for FastConformer."""

    def __init__(self, config: ParakeetFastConformerConfig):
        super().__init__()
        self.d_model = config.hidden_size
        self.scale_input = config.xscaling
        self.max_len = 5000
        self.pe = None

    def extend_pe(self, length: int, device: "torch.device", dtype: "torch.dtype"):
        """Reset and extend the positional encodings if needed."""
        needed_size = 2 * length - 1
        if hasattr(self, "pe") and self.pe is not None and self.pe.size(1) >= needed_size:
            return

        positions = torch.arange(length - 1, -length, -1, dtype=torch.float32, device=device).unsqueeze(1)
        self.create_pe(positions=positions, dtype=dtype)

    def create_pe(self, positions: "torch.Tensor", dtype: "torch.dtype"):
        """Create positional encoding matrix."""
        d_model = self.d_model
        pe = torch.zeros(positions.size(0), d_model, dtype=dtype, device=positions.device)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float32, device=positions.device) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(positions * div_term)
        pe[:, 1::2] = torch.cos(positions * div_term)

        # Register as buffer and add batch dimension
        pe_tensor = pe.unsqueeze(0)  # (1, T, D)
        try:
            self.register_buffer("pe", pe_tensor, persistent=False)
        except KeyError:
            del self.pe
            self.register_buffer("pe", pe_tensor, persistent=False)

    def forward(self, hidden_states: "torch.Tensor", cache_len: int = 0) -> tuple["torch.Tensor", "torch.Tensor"]:
        batch_size, seq_len, _ = hidden_states.shape
        input_len = seq_len + cache_len
        self.extend_pe(input_len, hidden_states.device, hidden_states.dtype)

        center_pos = self.pe.size(1) // 2 + 1
        start_pos = center_pos - input_len
        end_pos = center_pos + input_len - 1
        position_embeddings = self.pe[:, start_pos:end_pos]

        return position_embeddings


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class ParakeetFastConformerAttention(nn.Module):
    """Multi-head attention with relative positional encoding. See section 3.3 of https://huggingface.co/papers/1901.02860."""

    def __init__(self, config: ParakeetFastConformerConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        # W_{k,R} projection
        self.relative_k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)
        # global content bias
        self.bias_u = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))
        # global positional bias
        self.bias_v = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_embeddings: Optional[torch.Tensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        input_shape = hidden_states.shape[:-1]
        batch_size, seq_length = input_shape
        hidden_shape = (batch_size, seq_length, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if position_embeddings is not None:
            query_states_with_bias_u = query_states + self.bias_u.view(1, self.config.num_attention_heads, 1, self.head_dim)
            query_states_with_bias_v = query_states + self.bias_v.view(1, self.config.num_attention_heads, 1, self.head_dim)

            relative_key_states = self.relative_k_proj(position_embeddings)
            relative_key_states = relative_key_states.view(batch_size, relative_key_states.shape[1], self.config.num_attention_heads, self.head_dim)

            # terms (b) and (d)
            matrix_bd = query_states_with_bias_v @ relative_key_states.permute(0, 2, 3, 1)
            matrix_bd = self.rel_shift(matrix_bd)
            matrix_bd = matrix_bd[..., : seq_length]
            matrix_bd = matrix_bd * self.scaling

            if attention_mask is not None:
                if attention_mask.dtype == torch.bool:
                    matrix_bd.masked_fill_(attention_mask.logical_not(), float("-inf"))
                else:
                    matrix_bd = attention_mask + matrix_bd

        # will compute matrix_ac - terms (a) and (c) - and add matrix_bd
        attn_output, attn_weights = attention_interface(
            self,
            query_states_with_bias_u,
            key_states,
            value_states,
            matrix_bd if position_embeddings is not None else attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights

    def rel_shift(self, attention_scores):
        """Relative position shift for Shaw et al. style attention."""
        batch_size, num_heads, query_length, position_length = attention_scores.size()
        attention_scores = torch.nn.functional.pad(attention_scores, pad=(1, 0))
        attention_scores = attention_scores.view(batch_size, num_heads, -1, query_length)
        attention_scores = attention_scores[:, :, 1:].view(batch_size, num_heads, query_length, position_length)
        return attention_scores


# Feed Forward - using standard implementation
class ParakeetFastConformerFeedForward(nn.Module):
    def __init__(self, config: ParakeetFastConformerConfig):
        super().__init__()
        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.use_bias)
        self.activation = ACT2FN[config.hidden_act]
        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.use_bias)
        self.activation_dropout = config.activation_dropout

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.linear1(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.linear2(hidden_states)
        return hidden_states


# Convolution Module - FastConformer specific
class FastConformerConvModule(nn.Module):
    def __init__(self, config: ParakeetFastConformerConfig):
        super().__init__()
        hidden_size = config.hidden_size
        kernel_size = config.conv_kernel_size
        use_bias = config.use_bias

        assert (kernel_size - 1) % 2 == 0
        self.padding = (kernel_size - 1) // 2

        self.pointwise_conv1 = nn.Conv1d(hidden_size, hidden_size * 2, kernel_size=1, bias=use_bias)
        self.depthwise_conv = nn.Conv1d(
            hidden_size, hidden_size, kernel_size=kernel_size, padding=0, groups=hidden_size, bias=use_bias
        )
        self.batch_norm = nn.BatchNorm1d(hidden_size)
        self.activation = ACT2FN[config.hidden_act]
        self.pointwise_conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=1, bias=use_bias)

    def forward(self, hidden_states: torch.Tensor, pad_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        hidden_states = hidden_states.transpose(1, 2)
        hidden_states = self.pointwise_conv1(hidden_states)
        hidden_states = nn.functional.glu(hidden_states, dim=1)

        # Apply padding mask before convolution
        if pad_mask is not None:
            hidden_states = hidden_states.masked_fill(pad_mask.unsqueeze(1), 0.0)

        hidden_states = nn.functional.pad(hidden_states, (self.padding, self.padding))
        hidden_states = self.depthwise_conv(hidden_states)
        hidden_states = self.batch_norm(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.pointwise_conv2(hidden_states)
        return hidden_states.transpose(1, 2)


# Conformer Block
class ParakeetFastConformerBlock(nn.Module):
    def __init__(self, config: ParakeetFastConformerConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.gradient_checkpointing = False

        self.feed_forward1 = ParakeetFastConformerFeedForward(config)
        self.self_attn = ParakeetFastConformerAttention(config, layer_idx)
        self.conv = ParakeetFastConformerConvModule(config)
        self.feed_forward2 = ParakeetFastConformerFeedForward(config)

        self.norm_feed_forward1 = nn.LayerNorm(config.hidden_size)
        self.norm_self_att = nn.LayerNorm(config.hidden_size)
        self.norm_conv = nn.LayerNorm(config.hidden_size)
        self.norm_feed_forward2 = nn.LayerNorm(config.hidden_size)
        self.norm_out = nn.LayerNorm(config.hidden_size)

        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_embeddings: Optional[torch.Tensor] = None,
        pad_mask: Optional[torch.Tensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        # Store the original device for consistency
        original_device = hidden_states.device

        # First feed forward with 0.5 scaling
        ff1_output = self.feed_forward1(self.norm_feed_forward1(hidden_states))
        hidden_states = hidden_states + 0.5 * ff1_output.to(original_device)

        # Self attention
        normalized_hidden_states = self.norm_self_att(hidden_states)
        attn_output, _ = self.self_attn(
            hidden_states=normalized_hidden_states,
            attention_mask=attention_mask,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + attn_output.to(original_device)

        # Convolution
        conv_output = self.conv(self.norm_conv(hidden_states), pad_mask=pad_mask)
        hidden_states = hidden_states + conv_output.to(original_device)

        # Second feed forward with 0.5 scaling
        ff2_output = self.feed_forward2(self.norm_feed_forward2(hidden_states))
        hidden_states = hidden_states + 0.5 * ff2_output.to(original_device)

        # Final layer norm
        hidden_states = self.norm_out(hidden_states)

        return hidden_states


def calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num=1):
    """Calculates the output length of a Tensor passed through a convolution or max pooling layer"""
    add_pad: float = all_paddings - kernel_size
    one: float = 1.0
    for i in range(repeat_num):
        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one
        if ceil_mode:
            lengths = torch.ceil(lengths)
        else:
            lengths = torch.floor(lengths)
    return lengths.to(dtype=torch.int)


# Subsampling - reusing existing implementation
class ParakeetFastConformerSubsamplingConv2D(nn.Module):
    def __init__(self, config: ParakeetFastConformerConfig, feat_in: int):
        super().__init__()

        self.subsampling_factor = config.subsampling_factor
        self.conv_channels = config.subsampling_conv_channels

        self.num_layers = int(math.log2(self.subsampling_factor))
        self.stride = 2
        self.kernel_size = 3

        self.left_padding = (self.kernel_size - 1) // 2
        self.right_padding = (self.kernel_size - 1) // 2
        self.padding = self.left_padding
        self.ceil_mode = False

        layers = []
        in_channels = 1
        use_bias = True

        for i in range(self.num_layers):
            if i == 0:
                conv = nn.Conv2d(
                    in_channels,
                    self.conv_channels,
                    kernel_size=self.kernel_size,
                    stride=self.stride,
                    padding=self.padding,
                    bias=use_bias,
                )
                layers.append(conv)
                layers.append(nn.ReLU())
            else:
                depthwise_conv = nn.Conv2d(
                    self.conv_channels,
                    self.conv_channels,
                    kernel_size=self.kernel_size,
                    stride=self.stride,
                    padding=self.padding,
                    groups=self.conv_channels,
                    bias=use_bias,
                )
                pointwise_conv = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=1, bias=use_bias)
                layers.extend([depthwise_conv, pointwise_conv, nn.ReLU()])

        self.conv = nn.Sequential(*layers)

        in_length = torch.tensor(feat_in, dtype=torch.float)
        out_length = calc_length(
            lengths=in_length,
            all_paddings=self.left_padding + self.right_padding,
            kernel_size=self.kernel_size,
            stride=self.stride,
            ceil_mode=self.ceil_mode,
            repeat_num=self.num_layers,
        )

        if out_length.is_meta:
            out_length_val = feat_in // (self.stride**self.num_layers)
        else:
            out_length_val = int(out_length)

        self.out = nn.Linear(self.conv_channels * out_length_val, config.hidden_size, bias=True)

    def forward(self, input_features: torch.Tensor, attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        if attention_mask is not None:
            lengths = attention_mask.sum(-1)
        else:
            lengths = torch.full(
                (input_features.size(0),), input_features.size(1), dtype=torch.long, device=input_features.device
            )

        lengths = calc_length(
            lengths,
            all_paddings=self.left_padding + self.right_padding,
            kernel_size=self.kernel_size,
            stride=self.stride,
            ceil_mode=self.ceil_mode,
            repeat_num=self.num_layers,
        )

        hidden_states = input_features.unsqueeze(1)
        hidden_states = self.conv(hidden_states)

        batch_size, conv_channels, time_steps, freq_bins = hidden_states.size()
        hidden_states = self.out(hidden_states.transpose(1, 2).reshape(batch_size, time_steps, -1))

        max_audio_length = hidden_states.size(1)

        # Create masks
        pad_mask_valid = torch.arange(max_audio_length, device=hidden_states.device)[None, :] < lengths[:, None]
        pad_mask_for_att = pad_mask_valid.unsqueeze(1).expand(-1, max_audio_length, -1)
        pad_mask_for_att = pad_mask_for_att & pad_mask_for_att.transpose(1, 2)
        attention_mask = ~pad_mask_for_att
        pad_mask = ~pad_mask_valid
        attention_mask = pad_mask_valid

        return hidden_states, pad_mask_valid


# Base Model Classes
class ParakeetPreTrainedModel(PreTrainedModel):
    config_class = ParakeetFastConformerConfig
    base_model_prefix = "model"
    main_input_name = "input_features"
    supports_gradient_checkpointing = True
    _no_split_modules = ["FastConformerBlock"]
    _skip_keys_device_placement = []

    def _init_weights(self, module):
        # Get initializer_range from the appropriate config
        if hasattr(self.config, "initializer_range"):
            std = self.config.initializer_range
        elif hasattr(self.config, "fastconformer_config"):
            std = self.config.fastconformer_config.initializer_range
        else:
            std = 0.02  # default fallback
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            module.weight.data.fill_(1.0)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, FastConformerAttention):
            # Initialize positional bias parameters
            module.bias_u.data.normal_(mean=0.0, std=std)
            module.bias_v.data.normal_(mean=0.0, std=std)


class ParakeetFastConformerEncoder(ParakeetPreTrainedModel):
    def __init__(self, config: ParakeetFastConformerConfig):
        super().__init__(config)
        self.config = config
        self.gradient_checkpointing = False

        # TODO: to be renamed in config
        config.scale_input = True
        self.dropout = config.hidden_dropout_prob
        self.dropout_positions = config.dropout_emb
        self.layerdrop = config.encoder_layerdrop

        self.input_scale = math.sqrt(config.hidden_size) if config.scale_input else 1.0
        self.subsampling = ParakeetFastConformerSubsamplingConv2D(config, config.num_mel_bins)
        self.encode_positions = ParakeetFastConformerRelPositionalEncoding(config)

        self.layers = nn.ModuleList(
            [ParakeetFastConformerBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )  
    
    # @check_model_inputs
    @can_return_tuple
    def forward(
        self,
        input_features: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,  # should we rename to padding_mask?
    ) -> Union[tuple, BaseModelOutput]:

        hidden_states, attention_mask = self.subsampling(input_features, attention_mask)
        hidden_states = hidden_states * self.input_scale
        position_embeddings = self.encode_positions(hidden_states)

        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        position_embeddings = nn.functional.dropout(position_embeddings, p=self.dropout_positions, training=self.training)

        for encoder_layer in self.layers:
            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)
            to_drop = False
            if self.training:
                dropout_probability = torch.rand([])
                if dropout_probability < self.layerdrop:  # skip the layer
                    to_drop = True

            if to_drop:
                hidden_states = None
            else:
                hidden_states = encoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,
                    position_embeddings=position_embeddings,
                )

        return BaseModelOutput(last_hidden_state=hidden_states)


class ParakeetCTCDecoder(nn.Module):
    """
    CTC decoder for Parakeet models.

    This decoder implements Connectionist Temporal Classification (CTC) decoding
    for speech recognition. It consists of a linear projection layer that maps
    encoder hidden states to vocabulary logits.

    Args:
        config (ParakeetConfig): Configuration containing decoder parameters
    """

    def __init__(self, config: ParakeetConfig):
        super().__init__()
        self.config = config

        # CTC head - linear projection from encoder hidden size to vocabulary size
        self.ctc_head = nn.Linear(config.encoder_config.hidden_size, config.vocab_size)

        # Store CTC-specific parameters for easy access
        self.blank_token_id = config.blank_token_id
        self.ctc_loss_reduction = config.ctc_loss_reduction
        self.ctc_zero_infinity = config.ctc_zero_infinity

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through CTC decoder.

        Args:
            hidden_states: Encoder output of shape (batch_size, sequence_length, hidden_size)

        Returns:
            CTC logits of shape (batch_size, sequence_length, vocab_size)
        """
        return self.ctc_head(hidden_states)

    def compute_ctc_loss(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        input_lengths: torch.Tensor,
        label_lengths: torch.Tensor,
    ) -> torch.Tensor:
        """
        Compute CTC loss.

        Args:
            logits: Model predictions of shape (batch_size, sequence_length, vocab_size)
            labels: Target labels of shape (batch_size, max_label_length)
            input_lengths: Actual lengths of input sequences
            label_lengths: Actual lengths of label sequences

        Returns:
            CTC loss value
        """
        # Convert logits to log probabilities and transpose for CTC loss
        log_probs = F.log_softmax(logits, dim=-1)
        log_probs = log_probs.transpose(0, 1)  # (sequence_length, batch_size, vocab_size)

        # Prepare targets by removing padding and special tokens
        targets = []
        for i, label_length in enumerate(label_lengths):
            label = labels[i, :label_length]
            label = label[label != -100]  # Remove padding tokens
            targets.append(label)

        targets = torch.cat(targets)

        # Compute CTC loss
        loss = F.ctc_loss(
            log_probs=log_probs,
            targets=targets,
            input_lengths=input_lengths,
            target_lengths=label_lengths,
            blank=self.blank_token_id,
            reduction=self.ctc_loss_reduction,
            zero_infinity=self.ctc_zero_infinity,
        )

        return loss


class ParakeetPreTrainedModel(PreTrainedModel):
    config_class = ParakeetConfig
    base_model_prefix = "model"
    main_input_name = "input_features"
    supports_gradient_checkpointing = True
    _no_split_modules = ["FastConformerBlock"]
    _skip_keys_device_placement = []

    def _init_weights(self, module):
        # Get initializer_range from the encoder config
        std = self.config.encoder_config.initializer_range

        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            module.weight.data.fill_(1.0)
            if module.bias is not None:
                module.bias.data.zero_()


def calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num=1):
    """Calculates the output length of a Tensor passed through a convolution or max pooling layer"""
    add_pad: float = all_paddings - kernel_size
    one: float = 1.0
    for i in range(repeat_num):
        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one
        if ceil_mode:
            lengths = torch.ceil(lengths)
        else:
            lengths = torch.floor(lengths)
    return lengths.to(dtype=torch.int)


class ParakeetForCTC(ParakeetPreTrainedModel):
    """
    ParakeetCTC model for CTC-based speech recognition.

    This model follows an encoder-decoder architecture where:
    - Encoder: FastConformer model for processing audio features
    - Decoder: CTC decoder for speech recognition output

    Args:
        config (ParakeetConfig): Model configuration
    """

    def __init__(self, config: ParakeetConfig):
        super().__init__(config)

        # Encoder: FastConformer for audio feature processing
        self.encoder = ParakeetFastConformerEncoder(config.encoder_config)

        # Decoder: CTC decoder for speech recognition
        self.ctc_head = nn.Linear(config.encoder_config.hidden_size, config.vocab_size)

        # Store CTC-specific parameters for easy access
        self.blank_token_id = config.blank_token_id
        self.ctc_loss_reduction = config.ctc_loss_reduction
        self.ctc_zero_infinity = config.ctc_zero_infinity

        # Initialize weights
        self.post_init()

    def get_encoder(self):
        """Get the encoder component."""
        return self.encoder

    def set_encoder(self, encoder):
        """Set the encoder component."""
        self.encoder = encoder

    def get_decoder(self):
        """Get the decoder component."""
        return self.decoder

    def set_decoder(self, decoder):
        """Set the decoder component."""
        self.decoder = decoder

    @can_return_tuple
    def forward(
        self,
        input_features: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        input_lengths: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[tuple, CausalLMOutput]:
        return_dict = return_dict if return_dict is not None else self.config.encoder_config.use_return_dict

        # Forward through encoder
        encoder_outputs = self.encoder(
            input_features=input_features,
            attention_mask=attention_mask,
            # output_attentions=output_attentions,
            # output_hidden_states=output_hidden_states,
            # return_dict=True,
        )

        hidden_states = encoder_outputs.last_hidden_state

        # Forward through decoder to get CTC logits
        logits = self.ctc_head(hidden_states)

        loss = None
        if labels is not None:
            # Calculate encoder output lengths
            if input_lengths is not None:
                encoder_lengths = calc_length(
                    input_lengths.float(),
                    all_paddings=2,
                    kernel_size=3,
                    stride=2,
                    ceil_mode=False,
                    repeat_num=int(math.log2(self.config.encoder_config.subsampling_factor)),
                )
                encoder_lengths = encoder_lengths.long()
            elif attention_mask is not None:
                input_lens = attention_mask.sum(-1).float()
                encoder_lengths = calc_length(
                    input_lens,
                    all_paddings=2,
                    kernel_size=3,
                    stride=2,
                    ceil_mode=False,
                    repeat_num=int(math.log2(self.config.encoder_config.subsampling_factor)),
                )
                encoder_lengths = encoder_lengths.long()
            else:
                encoder_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=logits.device)

            # Calculate label lengths (excluding padding and blank tokens)
            label_lengths = torch.sum((labels != -100) & (labels != self.decoder.blank_token_id), dim=-1)

            # Compute CTC loss using the decoder
            loss = self.decoder.compute_ctc_loss(
                logits=logits,
                labels=labels,
                input_lengths=encoder_lengths,
                label_lengths=label_lengths,
            )

        return CausalLMOutput(
            loss=loss,
            logits=logits,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )

    def generate(
        self,
        input_features: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        input_lengths: Optional[torch.Tensor] = None,
    ) -> list[list[int]]:
        """
        Generate CTC decoded token sequences using greedy decoding.

        Args:
            input_features: Input mel-spectrogram features
            attention_mask: Attention mask
            input_lengths: Sequence lengths

        Returns:
            List of decoded token sequences (one per batch item)
        """
        with torch.no_grad():
            # Forward pass to get logits
            outputs = self.forward(
                input_features=input_features,
                attention_mask=attention_mask,
                input_lengths=input_lengths,
            )

            logits = outputs.logits  # (batch, time, vocab)

            # Greedy CTC decoding
            predicted_ids = torch.argmax(logits, dim=-1)  # (batch, time)

            batch_size = predicted_ids.size(0)
            decoded_sequences = []

            for batch_idx in range(batch_size):
                sequence = predicted_ids[batch_idx]

                # Get actual sequence length if available
                if input_lengths is not None:
                    # Calculate the actual output length after subsampling
                    actual_length = calc_length(
                        input_lengths[batch_idx : batch_idx + 1].float(),
                        all_paddings=2,
                        kernel_size=3,
                        stride=2,
                        ceil_mode=False,
                        repeat_num=int(math.log2(self.config.encoder_config.subsampling_factor)),
                    ).item()
                    sequence = sequence[:actual_length]
                elif attention_mask is not None:
                    # Use attention mask to determine length
                    input_len = attention_mask[batch_idx].sum().float()
                    actual_length = calc_length(
                        input_len.unsqueeze(0),
                        all_paddings=2,
                        kernel_size=3,
                        stride=2,
                        ceil_mode=False,
                        repeat_num=int(math.log2(self.config.encoder_config.subsampling_factor)),
                    ).item()
                    sequence = sequence[:actual_length]

                # CTC collapse: remove blanks and repeated tokens
                decoded_tokens = []
                prev_token = None

                for token_id in sequence.tolist():
                    # Skip blank tokens (using the decoder's blank token ID)
                    if token_id == self.decoder.blank_token_id:
                        prev_token = token_id
                        continue

                    # Skip repeated tokens (CTC collapse)
                    if token_id != prev_token:
                        decoded_tokens.append(token_id)

                    prev_token = token_id

                decoded_sequences.append(decoded_tokens)

            return decoded_sequences


__all__ = ["ParakeetForCTC", "ParakeetPreTrainedModel"]
